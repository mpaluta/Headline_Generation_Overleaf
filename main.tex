\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{acl2015}
%\usepackage{url}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{threeparttable}
\usepackage{algorithm2e}
\usepackage[toc,page]{appendix}

\title{This Time With Feeling: Sentiment to ML Headlines}

\author{Ben Arnoldy \\
  University of California, Berkeley \\
  {\tt arnoldyb@berkeley.edu} \\\And
  Mark Paluta \\
  University of California, Berkeley \\
  {\tt mpaluta@berkeley.edu} \\}
  
\date{July 2019}

\begin{document}

\maketitle

\begin{abstract}
    
\end{abstract}

\section{Introduction}
The art of good headline writing is not as simple as summarizing relevant details; it requires wit, originality, and a tone that matches the piece. Algorithms have made improvements in summarization, but still struggle with these more nuanced elements. Despite this shortcoming, algorithms could still be useful to an editor, suggesting a number of possible headlines to provide inspiration or for the editor to refine and synthesize into a final product. We propose an incremental improvement in automatic headline generation by incorporating tonal bias to match the underlying content.

\section{Background}
Headline generation is primarily approached as a single-document summarization task. The nature of headlines makes them relatively more difficult to produce with machine learning than typical document summaries. First, headlines tend to be shorter than the average sentence, requiring greater concision than typical ML summaries of one or more sentences. Additionally, they are often written in a short-hand style different from the body text, making extractive summarization theoretically less suitable than the more challenging task of abstractive summarization. 

Current methods of summarization involve an encoder-decoder architecture \citep{rush2015neural} with a ROUGE evaluation metric \cite{Ayana2017}. ROUGE is a recall-oriented metric to score a generated summary against a reference summary \cite{lin-2004-rouge}. Specific variants include ROUGE-N, scoring the number of common N-grams, and ROUGE-L, scoring the longest common subsequence. Recent refinements to headline generation include adversarial reward systems to combat repetition \cite{DBLP:journals/corr/abs-1902-07110} and applying state-of-the-art Transformer algorithms \cite{DBLP:journals/corr/abs-1901-07786}.

While most work on headline generation focuses exclusively on summarization, two recent papers in other summarization domains explored efforts to add desired sentiment. One naive approach from Chaudhari et al \cite{DBLP:journals/corr/abs-1802-09426} preprocessed sentences in the body of the corpus to generate sentence-level sentiment scores that characterize the emotional tone of the text. Sentences that did not match the desired sentiment polarity (positive or negative) were dropped before training the summarization model. Another more elaborate approach from Hu et al \cite{DBLP:journals/corr/HuYLSX17} to produce customer review summaries added a discriminator to evaluate the sentiment of generated summaries, looping back to the generator to optimize on this additional criterion. 

\section{Methodology}

\subsection{Objective}
We build a Universal Transformer architecture, initially basing our algorithm off of that of Gavrilov et al \cite{DBLP:journals/corr/abs-1901-07786}, but adding sentiment into the model in the spirit of Chaudhari \cite{DBLP:journals/corr/abs-1802-09426} and Hu \cite{DBLP:journals/corr/HuYLSX17}. To test the effectiveness of adding sentiment, we train an experimental and a control model.

The experimental model trains on the subset of the train dataset where the headline and the summary (i.e. lede) have the same sentiment polarity. This requirement results in a reduction in this model's training set from 990,858 articles to 749,745. To ensure that the control model trains on a comparable amount of data, we reduce the control dataset by randomly dropping articles by the same amount, 990,858 down to 749,745. 

With this approach modeled on Chaudari\cite{DBLP:journals/corr/abs-1802-09426}, our hypothesis is that the experimental model will show improved sentiment matching scores over those from the control model. We evaluate sentiment matching by measuring how many generated headlines have the same sentiment polarity as their underlying body texts, as well as measuring difference in sentiment scores between the generated and label headlines. We also compare ROUGE scores to see whether this method of improving sentiment matching comes at a cost or benefit to ROUGE score performance. 

\subsection{Workflow}

The workflow we defined to preprocess our data can be seen in Figure \ref{figure:preprocess}. Note that as we are running both an experimental group and a control group, the remainder of the workflow can be parallelized across two virtual machines.

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{HeadGen_pre_p.png}
  \caption{Preprocessing workflow}
  \label{figure:preprocess}
\end{figure*}

After preprocessing, the remaining pipeline consists of training, decoding on dev or test data, and scoring decoded output. See Figure \ref{figure:decode} for a full pipeline schematic.

\begin{figure*}
  \centering
  \includegraphics[width=\textwidth]{HeadGen_post_p.png}
  \caption{Train, decode, evaluate workflow}
  \label{figure:decode}
\end{figure*}

\subsection{Implementation}
Specifically, we chose to work with the package \href{https://github.com/tensorflow/tensor2tensor}{\texttt{tensor2tensor}}. This package was developed by the Google Brain team and is designed for furthering research on a variety of natural language processing problems\cite{vaswani-etal-2018-tensor2tensor}. We began by replicating the CNN/Daily Mail text summarization problem. We then adapted the architecture to accommodate a new problem of summarization, specifically generating headlines rather than key points, and using the New York Times annotated corpus rather than CNN/Daily Mail. We initially sought to imitate Gavrilov et al's architecture and results before embarking on our own modifications. The specific adaptations required in \texttt{tensor2tensor} to do this included:

\begin{itemize}
    \item Preprocessing of the New York Times corpus for \texttt{tensor2tensor} ingestion
    \item Define problem \texttt{Gavrilov} inheriting \texttt{Text2TextProblem}
    \item Apply a subword tokenizer to imitate Gavrilov et al's BPE tokenization
    \item Override a variety of default Universal Transformer hyperparameters as defined in Table \ref{table:hparams}
\end{itemize}

\subsection{Universal Transformer Architecture}
We use an encoder-decoder architecture, AdamW optimizer, and the set of specific hyperparameters outlined by Gavrilov et al \cite{DBLP:journals/corr/abs-1901-07786} as our baseline. Noteworthy parameters are listed in Table \ref{table:hparams}.

\begin{table}[h!]
\centering
\begin{small}
\begin{tabular}{|c |c |c|} 
 \hline
 Hyperparameter & Gavrilov & \_base \\ [0.5ex] 
 \hline
 Hidden layer size & 1024 & 1024 \\ 
 Filter size & 4096 & 4096 \\
 Heads of attention & 8 & 16 \\
 Layer preprocess sequence & None & normalize \\
 Layer postprocess sequence & dropout & dropout \\
 & $\rightarrow$ add $\rightarrow$ & $\rightarrow$ add \\
 & normalize & \\
 Postprocess dropout & 0.3 & 0.3 \\
 Encoder layers & 4 & 0 \\
 Decoder layers & 4 & 0 \\
 Learning rate warmup steps & 4000 & 8000 \\ [1ex]
 \hline
\end{tabular}
\end{small}
\caption{Hyperparameter Configurations}
\label{table:hparams}
\end{table}

\subsection{Corpus}
We use the \href{https://catalog.ldc.upenn.edu/LDC2008T19}{New York Times annotated corpus} to match Gavrilov. This contains 1.8 million New York Times articles spanning from 1987 to 2006. Like Gavrilov, we filtered out obituaries and kept articles where $20 \le wordcount \le 2000$ and $3 \le headline length \le 15$. We were left with 1,415,511 articles, which we then split along 70:10:20 proportions into train, dev, and test datasets. We also remove duplicative first paragraphs that occur in many of the articles. 

\subsection{Sentiment analysis}
We use a positive-negative polarity function as most research on sentiment is a simple positive-negative scale and we wish to leverage existing work. Following a model annotated by Speer\cite{RacistAI}, we take Liu's opinion lexicon\cite{Hu:2004:MSC:1014052.1014073} -- a list of around 6,800 words that are labeled either positive or negative -- and embed the lexicon's words in pre-trained GloVe embeddings. These embeddings and their [0,1] labels are used to train a logistic regression model that returns the log probability of a new word being positive and the log probability of it being negative. The negative probability is subtracted from the positive to arrive at a score; positive scores indicate positive associations and vice versa. To score an entire headline, we sum the individual scores of all the words in the headline and take an average. We construct a filter to remove articles where the polarity of the headline and its short summary differ, as shown in Figure \ref{figure:preprocess} ///write it as an xor///.

\subsection{Decoder}
Due to the cutting-edge nature of \texttt{tensor2tensor} including limited documentation, the decoder posed a number of initial challenges for us. As mentioned we initially modeled after Gavrilov et al's parameters. However, despite training to 25,000 steps, adjusting the beam-size between 1 and 10, $\alpha$ (length normalization) between .01 and 100, and the \texttt{decode\_variable} parameter between 1 and 100, we almost always got empty output from the decoder. We even adjusted from inputting 10 paragraphs to 3 and retrained entirely to try to downscope the problem and allow for faster convergence, but still encountered the problem. On rare occasions, we were able to get a few words repeated dozens of times. It was unclear to us the reason for our largely blank decoder output, but we suspect it was either a problem in our architecture construction or insufficient training time.

As a result, / ... /

\subsection{Evaluation}
To test the quality of generated headlines, we ran the top portion of the body text in all of the test dataset articles through the decoder to generate headlines to evaluate the quality of the sentiment-filtered model and the control model. Our evaluation included both the standard ROUGE metrics used for summarization tasks as well as two measures of sentiment we designed.

To compare the two models' performance on sentiment, we first calculate the percentage of generated headlines that match the polarity of the input body text. This percentage, which captures each model's ability to generate headlines that fit the sentiment of the their articles, is calculated as an f1-score where the headline's sentiment is the prediction for the true sentiment of the body. We expect that the preprocess filtering by sentiment will result in a trained model that produces higher f1-scores. Second, we subtract the difference in the predicted headline sentiment from the label headline sentiment and take an average across the dataset. This metric captures how well the models are able to generate headlines that match the sentiment of the label headlines. We expect that the experimental model will generate a smaller average sentiment differential than the control model, indicating a better sentiment match between predicted and label headlines.  

\section{Results}

An example input output pairing is shown in Table \ref{table:example}.

//// Replace with better output ////

\begin{table}[h!]
\centering
\begin{tiny}
\begin{tabular}{|p{7cm}|} 
 \hline
 \textbf{Input:} When Democratic Rep. Rashida Tlaib accused Republican Rep. Mark Meadows of racism earlier this year, one of his "best friends," House Oversight Committee chairman and Democratic Rep. Elijah Cummings immediately jumped to his defense. When Cummings was attacked on similar grounds by President Donald Trump over the weekend, it took a bit longer for Meadows to publicly repay the favor. On Saturday and Sunday, Trump went after Cummings on Twitter, calling the veteran Democratic lawmaker a "racist" and his district a "disgusting, rat and rodent infested mess." Trump's comments hung in the air for days, sparking yet another conversation about the President's race-focused rhetoric. Republicans largely stayed silent, including Meadows, whose warm relationship with Cummings prompted questions about his reticence to defend his friend. \\ [0.5ex] 
 \hline
 \textbf{Output:} Fatah Fatah Fatah Fatah Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Boys Boys Boys Boys Boys Boys Boys Boys Boys Boys Boys Fatah Fatah Fatah Fatah Fatah Boys Boys Boys Boys Boys Mitch Mitch Boys Boys Boys Mitch Mitch Mitch Fatah Fatah Fatah Boys Boys Mitch Mitch Mitch Mitch Mitch Mitch Boys Boys Boys Fatah Fatah Fatah Mitch Mitch Mitch Mitch Mitch Mitch Fatah Fatah Fatah Mitch Mitch Mitch Mitch Mitch Mitch Boys Boys Mitch Mitch Mitch Mitch Mitch Mitch Boys Boys Boys Boys Boys Boys Boys Boys Boys Boys Boys Boys Boys Boys Mitch Mitch Boys Boys Boys Boys Boys Mitch Mitch Boys Boys Fatah god god god god god god god god god god god Mitch Mitch Mitch god Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch Mitch  \\ 
 \hline
\end{tabular}
\end{tiny}
\caption{Example Output}
\label{table:example}
\end{table}

\subsection{ROUGE and sentiment scores}

We ran our algorithm for 25,000 steps as defined within \texttt{tensor2tensor}. For reference, the package recommended running machine translation tasks for approximately 300,000 steps to converge and obtain state-of-the-art BLEU scores. We were unable to run fully to convergence due to resource constraints. For further information about computation times, see Appendix A. Our loss curves are shown in Figure \ref{figure:loss}. Note that due to resource constraints we were also unable to continue evaluation on our dev data past 4000 steps due to the high computational cost of evaluation. We do note that the loss on our dev data was decreasing for the interval on which we were able to evaluate, which was an early indication that the algorithm was generalizing beyond the training data and not simply overfitting.

Decoding was also a computationally intensive process. 

\begin{figure}
  \centering
  \includegraphics[width=7.8cm]{loss.png}
  \caption{Loss curves}
  \label{figure:loss}
\end{figure}

Given the decoder challenges, we abandoned the Universal Transformer hyperparameters outlined in Gavrilov and instead worked with preset Universal Transformer hyperparameters provided in Tensor2Tensor. This resulted in decoder output good enough to obtain meaningful ROUGE metrics and sentiment metrics. 
////SHOW A BETTER DECODER OUTPUT/////

Results can be seen in Table \ref{table:results}

 \texttt{tensor2tensor}. This is estimated by [maybe some math here]

\begin{table*}[h!]
\begin{threeparttable}
\centering
\begin{small}
\begin{tabular}{|p{7cm}|p{.8cm}|p{.8cm}|p{.8cm}|p{.8cm}|p{.8cm}|p{.8cm}|p{1.3cm}|}
 \hline
 Model & R-1-f & R-1-r & R-2-f & R-2-r & R-L-f & R-L-r & \%-match \\
 \hline
 Gavrilov Universal Transformer - unsmoothed & \textbf{26.86} & 25.33 & \textbf{13.48} & \textbf{13.01} & \textbf{24.84} & 24.38 & - \\ [0.5ex] 
 Gavrilov baseline (first sentence) & 11.64 & \textbf{34.67} & 2.28 & 7.43 & 7.19 & \textbf{31.39} & - \\ 
 Our baseline (first 8 words) & 7.74 & 8.39 & 1.67 & 1.69 & 7.12 & 8.16 & 76 \\
 Our Universal Transformer - Sentiment-filtered & ?$^{*}$ & ?$^{*}$ & ?$^{*}$ & ?$^{*}$ & ?$^{*}$ & ?$^{*}$ & ?$^{*}$ \\
 Our Universal Transformer - Randomly filtered & 10.28$^{*}$ & 9.54$^{*}$ & 6.18$^{*}$ & 5.89$^{*}$ & 9.73$^{*}$ & 9.50$^{*}$ & 79$^{*}$ \\ [1ex]
 \hline
\end{tabular}
\end{small}
\begin{tablenotes}\footnotesize
\item[*] Note: not trained to convergence and scored on limited test set
\end{tablenotes}
\caption{Results}
\end{threeparttable}
\label{table:results}
\end{table*}

\section{Discussion}

/////As expected, we saw small improvements in both the ROUGE metrics and the sentiment metrics with the experimental model. This signals that the development of more complex integrations of sentiment into headline summarization models would be a worthwhile avenue for future research. //////

Although our methodology did show an improvement in our sentiment metric, we also note that this experiment required throwing out training data. In any real-world headline generation application, it is unlikely that one would want to throw out training data due. To measure the effect on ROUGE of throwing out this data, we initially wanted to also run a third model with no downsampling after the initial filtering. This would demonstrate the loss in ROUGE due to decreased training set size, but we did not have enough time to train this third model.

Discuss theories of what we are seeing in specific input-output example

\section{Conclusion}



\section{Materials}
Our materials for this project can be found at the following link:
\url{https://github.com/mpaluta/headline_generation}.

% "citing Rush here" \citep{rush2015neural}
% "citing Ayana here" \cite{Ayana2017}
% "citing Peng Xu here" \cite{DBLP:journals/corr/abs-1902-07110}
% "citing Gavrilov here" \cite{DBLP:journals/corr/abs-1901-07786}
% "citing Chaudari here" \cite{DBLP:journals/corr/abs-1802-09426}
% "citing Zhiting Hu here" \cite{DBLP:journals/corr/HuYLSX17}
% "citing Lin here" \cite{lin-2004-rouge}
% "citing Attention is all you Need here" \cite{DBLP:journals/corr/VaswaniSPUJGKP17}

\bibliographystyle{plain}
\bibliography{references}

\begin{appendices}

\section{Resource Considerations}

As we ran our algorithm on several different resource configurations, we decided to take the opportunity to record approximate computation times on various configurations to inform future resource requirements for future work, as some results were nonintuitive. Our approximate benchmarks can be seen in Table \ref{table:hardware}

\begin{table}[h!]
\begin{threeparttable}
\centering
\begin{small}
\begin{tabular}{|p{2cm}|p{2.2cm}|p{2cm}|} 
 \hline
 Task & Configuration & Approx. Computation Time \\ [0.5ex] 
 \hline\hline
 Vocab Generation & CPU-heavy$^{1}$ & 5 hr 40 min \\ 
 Vocab Generation & Balanced$^{2}$ & 40 min \\
 \hline
 One training step & CPU-heavy & 3.9 sec \\
 One training step & Balanced & 9.7 sec \\ 
 One training step & GPU-heavy$^{3}$ & I can do quick test for this later \\[1ex]
 \hline
\end{tabular}
\end{small}
\begin{tablenotes}\footnotesize
\item[1] 32 vCPUs on Google Cloud
\item[2] Nvidia 1060 GPU and Ryzen 7 2700 (8 cores, 16 threads)
\item[3] 8 vCPUs and Nvidia Tesla K80 on Google Cloud
\end{tablenotes}
\caption{Computational Benchmarks}
\end{threeparttable}
\label{table:hardware}
\end{table}

Of particular note, we observed that vocabulary generation was a GPU-intensive task and benefited greatly from the addition of a GPU. On the other hand, training was CPU-bottlenecked and even a competitive GPU with 8 vCPUs was outperformed by just 16 vCPUs with no GPU. This result in particular was surprising and led us to perform most of our intensive training on CPU-heavy configurations. It is also worth noting that the two configurations are of approximately equal cost given current Google Cloud pricing.

\end{appendices}

\end{document}



